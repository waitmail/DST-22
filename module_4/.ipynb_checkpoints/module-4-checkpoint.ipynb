{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "\n",
    "from pandas import Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score, auc,precision_score,recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-aa4f7ead66f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Размерность тренировочного датасета: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('/train.csv')\n",
    "df_test = pd.read_csv('/test.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('Размерность тренировочного датасета: ', df_train.shape)\n",
    "display(df_train.head(1))\n",
    "print('Размерность тестового датасета: ', df_test.shape)\n",
    "display(df_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Train'] = 1 # помечаем где у нас трейн\n",
    "df_test['Train'] = 0 # помечаем где у нас тест\n",
    "\n",
    "df = df_train.append(df_test, sort=False).reset_index(drop=True) # объединяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотрим на структуру объединенного датафрейма\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описания полей датасета\n",
    "\n",
    "* client_id - идентификатор клиента\n",
    "* education - уровень образования\n",
    "* sex - пол заемщика\n",
    "* age - возраст заемщика\n",
    "* car - флаг наличия автомобиля\n",
    "* car_type - флаг автомобиля иномарки\n",
    "* decline_app_cnt - количество отказанных прошлых заявок\n",
    "* good_work - флаг наличия “хорошей” работы\n",
    "* bki_request_cnt - количество запросов в БКИ\n",
    "* home_address - категоризатор домашнего адреса\n",
    "* work_address - категоризатор рабочего адреса\n",
    "* income - доход заемщика\n",
    "* foreign_passport - наличие загранпаспорта\n",
    "* sna - связь заемщика с клиентами банка\n",
    "* first_time - давность наличия информации о заемщике\n",
    "* score_bki - скоринговый балл по данным из БКИ\n",
    "* region_rating - рейтинг региона\n",
    "* app_date - дата подачи заявки\n",
    "* default - флаг дефолта по кредиту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тут что то не получилось с форматированием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотрим в каких колонках есть незаполненные значения\n",
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заполним колонку education MOD(ой)\n",
    "df['education'] = df['education'].fillna(df['education'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим списки колонок по типам \n",
    "#временные переменные\n",
    "date_cols = ['app_date']\n",
    "# бинарные переменные (default не включаем в список, посольку это наша целевая колонка)\n",
    "bin_cols = ['sex', 'car', 'car_type', 'foreign_passport', 'good_work']\n",
    "# категориальные переменные  (Train не включаем в список, это наш добавленныей признак, который не будет участвовать в обработке)\n",
    "cat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\n",
    "# числовые переменные, client_id не включаем\n",
    "num_cols = ['age', 'decline_app_cnt', 'bki_request_cnt','score_bki', 'income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображение информации о колонке\n",
    "def show_info_column(dataframe, column):\n",
    "    display(dataframe[column].describe())\n",
    "    plt.hist(dataframe[column], bins=30)\n",
    "    plt.ylabel('Количество')\n",
    "    plt.xlabel(column)\n",
    "    # plt.title('Histogram');\n",
    "    plt.show()\n",
    "    # Нижняя граница выбросов\n",
    "    minimum_emission_limit = round(\n",
    "        dataframe[column].mean() - 3 * dataframe[column].std(), 0)\n",
    "    # Верхняя граница выбросов\n",
    "    maximum_emission_limit = round(\n",
    "        dataframe[column].mean() + 3 * dataframe[column].std(), 0)\n",
    "    # Количество выбросов\n",
    "    quantity_of_emissions = len(dataframe[round(\n",
    "        ((df[column] - dataframe[column].mean()) / dataframe[column].std()).abs(), 0) > 3].index)\n",
    "    print('Границы выбросов', minimum_emission_limit, maximum_emission_limit)\n",
    "    display(sns.boxplot(data=dataframe[column]))\n",
    "    print('Количество выбросов', quantity_of_emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_logarifm(dataframe,column):\n",
    "    plt.figure()\n",
    "    sns.distplot(np.log1p(dataframe[column][dataframe['Train']==1]), kde = False, rug=False)    \n",
    "    plt.title(column)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Функция отображает confusion-матрицу\"\"\"\n",
    "    color_text = plt.get_cmap('GnBu')(1.0)\n",
    "    class_names = ['Дефолт', 'НЕ дефолт']\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm[0,0], cm[1,1] = cm[1,1], cm[0,0]\n",
    "    df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), title=\"Матрица ошибок\")\n",
    "    ax.title.set_fontsize(15)\n",
    "    sns.heatmap(df, square=True, annot=True, fmt=\"d\", linewidths=1, cmap=\"GnBu\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\", fontsize=12)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", rotation_mode=\"anchor\", fontsize=12)\n",
    "    ax.set_ylabel('Предсказанные значения', fontsize=14, color = color_text)\n",
    "    ax.set_xlabel('Реальные значения', fontsize=14, color = color_text)\n",
    "    b, t = plt.ylim()\n",
    "    plt.ylim(b+0.5, t-0.5)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(y_true, y_pred, y_pred_prob):\n",
    "    \"\"\"Функция выводит в виде датафрейма значения основных метрик классификации\"\"\"\n",
    "    dict_metric = {}\n",
    "    P = np.sum(y_true==1)\n",
    "    N = np.sum(y_true==0)\n",
    "    TP = np.sum((y_true==1)&(y_pred==1))\n",
    "    TN = np.sum((y_true==0)&(y_pred==0))\n",
    "    FP = np.sum((y_true==0)&(y_pred==1))\n",
    "    FN = np.sum((y_true==1)&(y_pred==0))\n",
    "    \n",
    "    dict_metric['P'] = [P,'Дефолт']\n",
    "    dict_metric['N'] = [N,'БЕЗ дефолта']\n",
    "    dict_metric['TP'] = [TP,'Истинно дефолтные']\n",
    "    dict_metric['TN'] = [TN,'Истинно НЕ дефолтные']\n",
    "    dict_metric['FP'] = [FP,'Ложно дефолтные']\n",
    "    dict_metric['FN'] = [FN,'Ложно НЕ дефолтные']\n",
    "    dict_metric['Accuracy'] = [accuracy_score(y_true, y_pred),'Accuracy=(TP+TN)/(P+N)']\n",
    "    dict_metric['Precision'] = [precision_score(y_true, y_pred),'Точность = TP/(TP+FP)'] \n",
    "    dict_metric['Recall'] = [recall_score(y_true, y_pred),'Полнота = TP/P']\n",
    "    dict_metric['F1-score'] = [f1_score(y_true, y_pred),'Среднее гармоническое Precision и Recall']\n",
    "    dict_metric['ROC_AUC'] = [roc_auc_score(y_true, y_pred_prob),'ROC-AUC']    \n",
    "\n",
    "    temp_df = pd.DataFrame.from_dict(dict_metric, orient='index', columns=['Значение', 'Описание метрики'])\n",
    "    display(temp_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_roc_curve(y_true, y_pred_prob):\n",
    "    \"\"\"Функция отображает ROC-кривую\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], label='Случайный классификатор', linestyle='--')\n",
    "    plt.plot(fpr, tpr, label = 'Логистическая регрессия')\n",
    "    plt.title('Логистическая регрессия ROC AUC = %0.3f' % roc_auc_score(y_true, y_pred_prob))\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info_column(df,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(df['age'][df['Train']==1], kde = False, rug=False)    \n",
    "plt.title('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(np.log1p(df['age'][df['Train']==1]), kde = False, rug=False)    \n",
    "plt.title('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#После логорифмирование выгляднет лучше, берем логарифм от признака  \n",
    "df['age'] = np.log1p(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info_column(df,'decline_app_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()\n",
    "df_copy['decline_app_cnt'] = np.log1p(df_copy['decline_app_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_info_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-74a7e4bf0c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_info_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_copy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'decline_app_cnt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'show_info_column' is not defined"
     ]
    }
   ],
   "source": [
    "show_info_column(df_copy,'decline_app_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#По прежнему очень много выбросов, возьмем логарифм , а выбросы пока оставим как есть \n",
    "df['decline_app_cnt'] = np.log1p(df['decline_app_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info_column(df,'bki_request_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()\n",
    "df_copy['bki_request_cnt'] = np.log1p(df_copy['bki_request_cnt'])\n",
    "show_info_column(df_copy,'bki_request_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#График выглядет лучше , но выбросов стало очень много, возьмем пока логарфим от признака и выбросы оставим\n",
    "df['bki_request_cnt'] = np.log1p(df['bki_request_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info_column(df,'score_bki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы видим нормальное распреление, небольшое количество выбросов , поэтому оставляем все как есть "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info_column(df,'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.copy()\n",
    "df_copy['income'] = np.log1p(df_copy['income'])\n",
    "show_info_column(df_copy,'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#График стал лучше, берем логарифм от признака\n",
    "df['income'] = np.log1p(df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(df[num_cols][df['Train']==1].corr(), square=True,\n",
    "              annot=True, fmt=\".3f\", linewidths=0.1, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мультиколлинеарности не обнаружено, оставляем все признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df['Train']==1]\n",
    "imp_num = Series(f_classif(temp_df[num_cols], temp_df['default'])[0], index = num_cols)\n",
    "imp_num.sort_values(inplace = True)\n",
    "imp_num.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in bin_cols:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "edu_label_encoder = LabelEncoder()\n",
    "df['education'] = edu_label_encoder.fit_transform(df['education'])\n",
    "\n",
    "# убедимся в преобразовании    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df['Train']==1]\n",
    "imp_cat = Series(mutual_info_classif(temp_df[bin_cols + cat_cols], temp_df['default'],\n",
    "                                     discrete_features =True), index = bin_cols + cat_cols)\n",
    "imp_cat.sort_values(inplace = True)\n",
    "imp_cat.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым значимым признаком по Mutual information тесту является связь заемщика с клиентами банка (sna) и давность наличия информации о заемщике (first_time), потом идет рейтинг региона (region_rating) и в конце пол (sex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['app_date','client_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем категориальные признаки\n",
    "df=pd.get_dummies(df, prefix=cat_cols, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Стандартизация пока пропустим\n",
    "#X_num_train = StandardScaler().fit_transform(df[df['Train']==1][num_cols].values)\n",
    "#Y_num_train = StandardScaler().fit_transform(df[df['Train']==0][num_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.query('Train == 1').drop(['Train'], axis=1)\n",
    "test_data = df.query('Train == 0').drop(['Train'], axis=1)\n",
    "\n",
    "y = train_data.default.values            # наш таргет\n",
    "X = train_data.drop(['default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:,1]\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оценка качества модели\n",
    "all_metrics(y_test, y_pred, probs)\n",
    "show_roc_curve(y_test, probs)\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем GridSearch на небольшом кол-ве итераций max_iter=50 и с достаточно большой дельтой останова tol1e-3\n",
    "# чтобы получить оптимальные параметры модели в первом приближении\n",
    "model = LogisticRegression(random_state=RANDOM_SEED)\n",
    "iter_ = 50\n",
    "epsilon_stop = 1e-3\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], \n",
    "     'solver': ['liblinear', 'lbfgs'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['l2'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['none'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "]\n",
    "gridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=5)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "model = gridsearch.best_estimator_\n",
    "##печатаем параметры\n",
    "best_parameters = model.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    ##печатаем метрики\n",
    "preds = model.predict(X_test)\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, preds))\n",
    "print('Precision: %.4f' % precision_score(y_test, preds))\n",
    "print('Recall: %.4f' % recall_score(y_test, preds))\n",
    "print('F1: %.4f' % f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    random_state=RANDOM_SEED, \n",
    "    C=1.0,\n",
    "\tclass_weight= 'balanced',\n",
    "\tdual= False,\n",
    "\tfit_intercept= True,\n",
    "\tintercept_scaling= 1,\n",
    "\tl1_ratio= None,\n",
    "\tmax_iter= 50,\n",
    "\tmulti_class= 'auto',\n",
    "\tn_jobs= None,\n",
    "\tpenalty= 'none',\n",
    "\tsolver= 'sag',\n",
    "\ttol= 0.001,\n",
    "\tverbose= 0,\n",
    "\twarm_start= False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:,1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оценка качества модели\n",
    "all_metrics(y_test, y_pred, probs)\n",
    "show_roc_curve(y_test, probs)\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим метрики стали значительно лучше чем у первой модели с параметрами по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.query('Train == 1').drop(['Train'], axis=1)\n",
    "test_data = df.query('Train == 0').drop(['Train'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data.drop(['default'], axis=1)\n",
    "y_train = train_data.default.values\n",
    "X_test = test_data.drop(['default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    random_state=RANDOM_SEED, \n",
    "    C=1.0,\n",
    "\tclass_weight= 'balanced',\n",
    "\tdual= False,\n",
    "\tfit_intercept= True,\n",
    "\tintercept_scaling= 1,\n",
    "\tl1_ratio= None,\n",
    "\tmax_iter= 50,\n",
    "\tmulti_class= 'auto',\n",
    "\tn_jobs= None,\n",
    "\tpenalty= 'none',\n",
    "\tsolver= 'sag',\n",
    "\ttol= 0.001,\n",
    "\tverbose= 0,\n",
    "\twarm_start= False)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "submit = pd.DataFrame(df_test.client_id)\n",
    "submit['default']=y_pred_prob\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
